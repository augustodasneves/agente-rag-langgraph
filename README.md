# RAG Agent Pro üöÄ

Este projeto √© um agente de **Retrieval-Augmented Generation (RAG)** constru√≠do com **LangGraph**, **LangChain**, e **Ollama**. O agente permite que os usu√°rios fa√ßam o upload de documentos PDF, os indexem em um banco de dados vetorial **pgvector**, e realizem perguntas cujas respostas s√£o baseadas exclusivamente no conte√∫do desses documentos.

---

## üèóÔ∏è Arquitetura

O sistema √© composto por quatro servi√ßos principais orquestrados via Docker:

1.  **UI (Streamlit)**: Interface de chat e upload de arquivos.
2.  **Agent (FastAPI + LangGraph)**: O c√©rebro do sistema, que coordena o fluxo de consulta (recupera√ß√£o e gera√ß√£o).
3.  **Vector DB (PostgreSQL + pgvector)**: Armazenamento vetorial para busca sem√¢ntica de fragmentos de documentos.
4.  **LLM/Embeddings (Ollama)**: Motor de infer√™ncia local que executa o modelo Llama 3.2 para gera√ß√£o de texto e embeddings.

### Diagrama de Blocos

```mermaid
graph TD
    User([Usu√°rio]) <--> UI[Streamlit UI]
    UI <--> Agent[FastAPI Agent]
    Agent <--> VDB[(PostgreSQL + pgvector)]
    Agent <--> LLM[Ollama Llama 3.2]
```

---

## üîÑ Fluxo de Funcionamento (Diagrama de Sequ√™ncia)

O diagrama abaixo ilustra o processamento de uma pergunta pelo agente:

```mermaid
sequenceDiagram
    participant U as Usu√°rio
    participant S as Streamlit UI
    participant A as Agent (LangGraph)
    participant DB as Vector DB (pgvector)
    participant O as Ollama (LLM)

    U->>S: Digita uma pergunta
    S->>A: POST /ask {question}
    
    Note over A: In√≠cio do Grafo (LangGraph)
    A->>DB: Busca por trechos similares (Retrieve)
    DB-->>A: Retorna fragmentos de texto
    
    A->>O: Gera resposta baseada no contexto (Generate)
    O-->>A: Retorna a resposta final do LLM
    
    Note over A: Fim do Grafo
    
    A-->>S: Retorna JSON {answer}
    S->>U: Exibe a resposta na tela
```

---

## üõ†Ô∏è Tecnologias Utilizadas

-   **Backend**: Python, FastAPI, LangGraph.
-   **LangChain**: Gerenciamento de cadeias, prompts e integra√ß√µes de LLM.
-   **Banco de Dados**: PostgreSQL com a extens√£o `pgvector`.
-   **LLM Local**: Ollama (Rodando o modelo `llama3.2`).
-   **Frontend**: Streamlit.
-   **Infraestrutura**: Docker & Docker Compose.

---

## üöÄ Como Rodar Localmente (Docker)

### Pr√©-requisitos

-   [Docker](https://docs.docker.com/get-docker/) instalado.
-   [Docker Compose](https://docs.docker.com/compose/install/) instalado.

### Passo a Passo

1.  **Clonar o reposit√≥rio**:
    ```bash
    git clone <url-do-repositorio>
    cd agente-rag-langgraph
    ```

2.  **Configurar vari√°veis de ambiente** (opcional):
    O sistema j√° vem com valores padr√£o, mas voc√™ pode criar um arquivo `.env` baseado no `.env.example` se desejar customizar senhas ou portas.

3.  **Subir os servi√ßos**:
    ```bash
    docker compose up --build
    ```

    > [!IMPORTANT]
    > Na primeira execu√ß√£o, o servi√ßo do Ollama ir√° baixar o modelo `llama3.2` (aprox. 2GB). O agente aguardar√° o download ser conclu√≠do para ficar dispon√≠vel (`healthcheck`).

4.  **Acessar as interfaces**:
    -   **Interface do Chat (UI)**: [http://localhost:8501](http://localhost:8501)
    -   **Documenta√ß√£o da API**: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## üìÇ Funcionalidades

-   **Upload de PDF**: Processa e indexa documentos automaticamente.
-   **Busca Sem√¢ntica**: Recupera apenas as partes relevantes dos documentos.
-   **Privacidade**: Tudo roda localmente via Docker e Ollama, sem envio de dados para APIs externas.
-   **Interface intuitiva**: Experi√™ncia de chat moderna via Streamlit.
